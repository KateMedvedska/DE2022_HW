# Фінальний проект

## Передумови

1. Файли з даними для виконання проекту знаходяться в директорії `lect_07/data-final-project`
2. Був створений бакет в GCS для зберігання сирих файлів (далі - raw-бакет), файли скопійовані в нього.
3. В Bigquery створені наступні датасети: `bronze`, `silver`, `gold`
4. Оркестрація віддбувається з локально 
запущенного Apache Airflow. Будь-яке переміщення чи трансформація даних керується
за допомогою Apache Airflow.


## Легенда

Ви працюєте в Data відділі компанії, що займається продажем побутової електроніки.
Ваш відділ отримав завдання проаналізувати продажі за географічною
локацією покупців (по штатах) та за віком.
Для виконання цієї задачі вам вдалось домовитись про інтеграцію 2 джерел данних: `customers` та `sales`.
Для цього вам потрібно розробити 2 пайплайни даних:

## 1. `process_sales` pipeline

Ви вирішили вставити дані з raw-бакета в `bronze` за допомогою читання
файлу як зовнішньої таблиці  (schema-on-read), адже формат файлу `.csv`,
і такий спосіб є найбезпечнішим. Схему ви вирішили накласти
так, щоб будь-яке поле у файлі вважати STRING. В таблиці `sales` 
в датасеті `bronze` всі поля теж STRING.

Назви колонок в `bronze` ви вирішили залишити з файлу джерела
для того, щоб полегшити пошук помилок, а саме:
```
CustomerId, PurchaseDate, Product, Price
```

При трансфері данних в `silver` ви помітили, що дані мають деякий "бруд"
і вирішили їх почистити.

Схема в `silver` вже має ті типи, які зручні для дослідження даних.
Колонки мають назви згідно правил, прийнятих в компанії, а саме:

```
client_id, purchase_date, product_name, price
```

Дані ви вирішили партиціонувати, так як даних багато і аналітикам
потрібно робити виборки відштовхуючись від дати.

Дані вже приходять партиціоновані - окрема дата в окремій папці.

## 2. `process_customers` pipeline

Це джерело даних приходить дивним чином:
постачальник даних вирішив не погодився класти дані за кожен день
в окрему папку, натомість він кожен зливає весь дамп таблиці кожного дня.
Тобто, кожен наступний день містить дані за всі попередні дні.

Дані ви вирішуєте не партиціонувати, адже даних небагато.


Ви прокидуєте дані на рівні `bronze` та `silver`.

В `silver` таблиця має наступні колонки:
```
client_id, first_name, last_name, email, registration_date, state
```

В `bronze` назви колонки лишаються оригінальні (такі, як в CSV файлі)


-----------------------------------------------------------------------------
Ви запускаєте обидва DAGи, всі дані трансферяться успішно.

Аналітики аналізують дані в `silver` і помічають, що клієнти не заповнювали
деякі дані, тому вони порожні. Найважливіще - це назви штатів, бо
без них не можна робити аналітику по географічному розташуванню.
А дані про вік покупців взагалі відсутні.
Також, в планах компанії є впровадження нотифікацій покупців,
а записи імен та прізвищ заповнення не всюди (користувачі мають звичку
заповнювати або імʼя, або прізвище вибірково)

Щоб вирішити описані вище проблеми, ви домовляєтесь про інтеграцію
третього джерела даних: `user_profiles`:


## 3. 'process_user_profiles' pipeline

Ці дани постачальник доставляє в форматі JSONLine. Ви будуєте пайплайн і
трансферите їх до рівня `silver`. Дані мають ідеальну якість.

Даний пайплайн запускається вручну, а не за розкладом.

Після того, як дані всі процесяться успішно, треба збагатити дані
`customers` за допомогою даних з `user_profiles`.

## 4. 'enrich_user_profiles' pipeline

Це єдини пайплайн, який пише дані в рівень `gold`. 
В датасеті `gold` має створюватись таблиця `user_profiles_enriched` 
в результаті роботи цього пайплайну. Ця таблиця містить всі дані з таблиці 
`silver.customers`, але мають бути заповненні всі імена, прізвища, штати
даними з таблиці `silver.user_profiles`. Також додатково потрібно дадати
всі поля, які є в `silver.user_profiles` та не вистачає в `silver.customers`
(наприклад, `phone_number`). 

Цей пайплайн теж має запускатись вручну (не за розкладом).

Реалізовано, щоб 'process_customers' після успішної обробки данних запускав 'enrich_user_profiles'



В решті-решт, коли всі пайплайни побудовані, можемо дати відповідь на 
наступну аналітичну задачу:

> В якому штаті було куплено найбільше телевізорів покупцями від 20 до 30 років за першу декаду вересня?
```
Відповіть: у штаті Idaho було куплено 179 телевізоврів.
```